{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "157ea9f4",
      "metadata": {
        "id": "157ea9f4"
      },
      "source": [
        "# STEP 2: Convert Text ‚Üí Numbers (Tokenization & Vocabulary)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Neural Network can not understand the text it only understans the numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7c7bd1f",
      "metadata": {
        "id": "f7c7bd1f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv(\"data/training.csv\")\n",
        "val_df = pd.read_csv(\"data/validation.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62d92ca",
      "metadata": {
        "id": "c62d92ca"
      },
      "source": [
        "## Define a Simple Tokenizer\n",
        "Below function will do this :\n",
        "I am Tirth Patel  -- > ['i', 'am', 'tirth', 'patel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e98827",
      "metadata": {
        "id": "92e98827"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text.split()\n",
        "\n",
        "print(tokenize(\"I am Tirth Patel\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44bdd5d9",
      "metadata": {
        "id": "44bdd5d9"
      },
      "source": [
        "## Build Vocabulary from Training Data\n",
        "\n",
        "üìå Why special tokens?\n",
        "- `<PAD>` : for padding\n",
        "- `<UNK>` : unseen words in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3d4721",
      "metadata": {
        "id": "fc3d4721"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counter = Counter()\n",
        "\n",
        "for sentance in train_df[\"text\"]:\n",
        "    tokens = tokenize(sentance)\n",
        "    word_counter.update(tokens)\n",
        "\n",
        "\n",
        "# Special tokens\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "vocab = {\n",
        "    PAD_TOKEN: 0,\n",
        "    UNK_TOKEN: 1\n",
        "}\n",
        "\n",
        "for word,_ in word_counter.items():\n",
        "     vocab[word] = len(vocab)\n",
        "\n",
        "print(\"Vocabulary size:\",len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafbafdd",
      "metadata": {
        "id": "fafbafdd"
      },
      "source": [
        "## Convert Text to Numerical Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708f3ecd",
      "metadata": {
        "id": "708f3ecd"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentance , vocab) :\n",
        "     tokens = tokenize(sentance)\n",
        "     return [vocab.get(token, vocab[UNK_TOKEN]) for token in tokens]\n",
        "\n",
        "print(encode_sentence(\"I feel happy\", vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb224c07",
      "metadata": {
        "id": "cb224c07"
      },
      "source": [
        "## Padding (VERY IMPORTANT)\n",
        "Neural networks need same-length inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8719e204",
      "metadata": {
        "id": "8719e204"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 50\n",
        "\n",
        "def pad_suquence(seq , max_len) :\n",
        "     if len(seq) < max_len:\n",
        "        return seq + [vocab[PAD_TOKEN]] * (max_len - len(seq))\n",
        "     else:\n",
        "          return seq[:max_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcbd3637",
      "metadata": {
        "id": "bcbd3637"
      },
      "source": [
        "## Final Encoded Dataset (Training Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c45ee82b",
      "metadata": {
        "id": "c45ee82b"
      },
      "outputs": [],
      "source": [
        "X_train = [\n",
        "     pad_suquence(encode_sentence(text , vocab) , MAX_LEN)\n",
        "     for text in train_df['text']\n",
        "]\n",
        "\n",
        "y_train = train_df['label'].values\n",
        "print(X_train[0])\n",
        "print(len(X_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d8442d",
      "metadata": {
        "id": "83d8442d"
      },
      "source": [
        "## What You‚Äôve Achieved\n",
        "Implemented custom tokenization, vocabulary construction, sequence encoding, and padding for NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93dda1ce",
      "metadata": {
        "id": "93dda1ce"
      },
      "source": [
        "# STEP 3: PyTorch Dataset & DataLoader (Industry Standard)\n",
        "‚ùó Goal of this step\n",
        "> Convert your processed data into a format that PyTorch models can train on.\n",
        "---\n",
        "## Why Dataset & DataLoader?\n",
        "Instead of loading everything at once, PyTorch:\n",
        "- Loads data in batches\n",
        "- Shuffles training data\n",
        "- Works efficiently on CPU / GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561d9eff",
      "metadata": {
        "id": "561d9eff"
      },
      "source": [
        "## Convert Data to PyTorch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4947ce12",
      "metadata": {
        "id": "4947ce12"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce92e0af",
      "metadata": {
        "id": "ce92e0af"
      },
      "source": [
        "## Create a Custom Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9d029a",
      "metadata": {
        "id": "8c9d029a"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset) :\n",
        "     def __init__(self , texts , labels) :\n",
        "          self.texts = torch.tensor(texts, dtype=torch.long)\n",
        "          self.labels = torch.tensor(labels ,dtype=torch.long )\n",
        "\n",
        "     def __len__(self) :\n",
        "          return len(self.labels)\n",
        "\n",
        "     def __getitem__(self , idx) :\n",
        "          return self.texts[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2cefdd7",
      "metadata": {
        "id": "b2cefdd7"
      },
      "source": [
        "## Create Training & Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3333c33a",
      "metadata": {
        "id": "3333c33a"
      },
      "outputs": [],
      "source": [
        "train_dataset = EmotionDataset(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XBbnnPqUf-rU",
      "metadata": {
        "id": "XBbnnPqUf-rU"
      },
      "outputs": [],
      "source": [
        "X_val = [\n",
        "    pad_suquence(encode_sentence(text,vocab), MAX_LEN)\n",
        "    for text in val_df[\"text\"]\n",
        "]\n",
        "\n",
        "y_val = val_df[\"label\"].values\n",
        "val_dataset = EmotionDataset(X_val, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nAHJBytrhTF5",
      "metadata": {
        "id": "nAHJBytrhTF5"
      },
      "source": [
        "## Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-akxLaq-hTuM",
      "metadata": {
        "id": "-akxLaq-hTuM"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1lQX4k6rhjnE",
      "metadata": {
        "id": "1lQX4k6rhjnE"
      },
      "source": [
        "## Sanity Check (IMPORTANT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-B_mrFbFhkLi",
      "metadata": {
        "id": "-B_mrFbFhkLi"
      },
      "outputs": [],
      "source": [
        "for batch_texts, batch_labels in train_loader:\n",
        "    print(batch_texts.shape)\n",
        "    print(batch_labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zdcAp7G6h5tr",
      "metadata": {
        "id": "zdcAp7G6h5tr"
      },
      "source": [
        "# STEP 4: Build the Deep Neural Network (Embedding + LSTM)\n",
        "\n",
        "## Key Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EKz2coVLh6KZ",
      "metadata": {
        "id": "EKz2coVLh6KZ"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "NUM_CLASSES = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-Z2HyIUdiLFr",
      "metadata": {
        "id": "-Z2HyIUdiLFr"
      },
      "source": [
        "## Define the Model (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iHEdA3vViJd3",
      "metadata": {
        "id": "iHEdA3vViJd3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EmotionLSTM(nn.Module) :\n",
        "    def __init__(self ,vocab_size , embed_dim , hidden_dim , num_classes) :\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self , x ) :\n",
        "        emb = self.emb(x)\n",
        "        _, (hidden, _) = self.lstm(emb)\n",
        "        out = self.fc(hidden[-1])\n",
        "        return out\n",
        "\n",
        "model = EmotionLSTM(\n",
        "    VOCAB_SIZE,\n",
        "    EMBED_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    NUM_CLASSES\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8SsKMDZqit-O",
      "metadata": {
        "id": "8SsKMDZqit-O"
      },
      "source": [
        "## Sanity Check (CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hQicbg2QiulZ",
      "metadata": {
        "id": "hQicbg2QiulZ"
      },
      "outputs": [],
      "source": [
        "for texts, labels in train_loader:\n",
        "    outputs = model(texts)\n",
        "    print(outputs.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UHt2rjwyi8kG",
      "metadata": {
        "id": "UHt2rjwyi8kG"
      },
      "source": [
        "# üü¢ STEP 5: Training the Model (Learning Happens Here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nJLXmMLVi9Jl",
      "metadata": {
        "id": "nJLXmMLVi9Jl"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "EPOCHS = 15\n",
        "for i in range(EPOCHS) :\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for texts,labels in train_loader :\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        output = model(texts)\n",
        "        loss = loss_fn(output , labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {i+1}/{EPOCHS}, Loss: {total_loss/len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJbWEs6FkcOl",
      "metadata": {
        "id": "oJbWEs6FkcOl"
      },
      "source": [
        "## Validation Loop (VERY IMPORTANT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O85T2Mnbjz-g",
      "metadata": {
        "id": "O85T2Mnbjz-g"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in val_loader:\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(texts) # Corrected: 'ouput' to 'output'\n",
        "        predictions = torch.argmax(output, dim=1) # Corrected: used 'output' instead of global 'outputs'\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy * 100} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cJsxYZWJlNHt",
      "metadata": {
        "id": "cJsxYZWJlNHt"
      },
      "source": [
        "# 2Ô∏è‚É£ Test on Real Human Sentences (MOST IMPORTANT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VSgF3-ArktLD",
      "metadata": {
        "id": "VSgF3-ArktLD"
      },
      "outputs": [],
      "source": [
        "def predict_emotion(text , model , vocab ) :\n",
        "    model.eval()\n",
        "\n",
        "    encoded = encode_sentence(text, vocab)\n",
        "    padded = pad_suquence(encoded, MAX_LEN) # Corrected: pad_sequence to pad_suquence\n",
        "\n",
        "    input_tensor = torch.tensor([padded], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        predicted_class = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "print(predict_emotion(\"I feel lonely and exhausted\", model, vocab))\n",
        "# print(predict_emotion(\"Today is the best day of my life\", model, vocab))\n",
        "print(predict_emotion(\"I feel like a dog\", model, vocab))\n",
        "print(predict_emotion(\"I am feeling very happy today and everything is going great.\", model, vocab))\n",
        "print(predict_emotion(\"This is the best moment of my life, I cannot stop smiling.\", model, vocab))\n",
        "print(predict_emotion(\"I feel empty and tired, nothing seems to matter anymore.\", model, vocab))\n",
        "print(predict_emotion(\"This is so unfair, I cannot tolerate this anymore.\", model, vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AvV9FnP3mBtN",
      "metadata": {
        "id": "AvV9FnP3mBtN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
